# pylint: disable=line-too-long
# ruff: noqa: E501
"""
https://manus.im/blog/introducing-wide-research
"""

import asyncio
import json
import pathlib
import traceback
import re
import uuid
import yaml
import httpx
import httpcore

from agents import function_tool

from utu.agents import SimpleAgent
from utu.config import ConfigLoader
from utu.tools import SearchToolkit, SerperToolkit
from utu.utils import AgentsUtils, FileUtils
from utu.utils.Citation import CitationProcessor

PROMPTS = FileUtils.load_yaml(pathlib.Path(__file__).parent / "prompts.yaml")
SEARCH_TOOLKIT = SearchToolkit(ConfigLoader.load_toolkit_config("search"))
SERPER_TOOLKIT = SerperToolkit(ConfigLoader.load_toolkit_config("serper"))
CONCURRENCY = 20


def parse_document(text):
    """
    è§£æ YAML æ ¼å¼çš„æ–‡æ¡£
    
    Args:
        text: YAML æ ¼å¼çš„æ–‡æœ¬
        
    Returns:
        dict: è§£æåçš„å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µ
    """
    try:
        # ç›´æ¥ç”¨ yaml.safe_load è§£æ
        data = yaml.safe_load(text)
        return data if data else {}
    except yaml.YAMLError as e:
        print(f"YAML è§£æé”™è¯¯: {e}")
        return {}

def parse_markdown_outline(markdown_text: str) -> dict:
    """
    Parses a structured Markdown outline into a nested dictionary.
    """
    lines = markdown_text.strip().split('\n')
    
    outline = {"title": "", "sections": []}
    current_section = None
    
    # Use a regex to find FOCUS, KEYWORDS, TABLE_RECOMMENDED and TABLE_NUMBER lines
    focus_regex = re.compile(r"^\s*FOCUS:\s*(.*)", re.IGNORECASE)
    keywords_regex = re.compile(r"^\s*KEYWORDS:\s*(.*)", re.IGNORECASE)
    table_regex = re.compile(r"^\s*TABLE_RECOMMENDED:\s*(.*)", re.IGNORECASE)
    table_number_regex = re.compile(r"^\s*TABLE_NUMBER:\s*(.*)", re.IGNORECASE)
     
    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Main Title
        if line.startswith('# '):
            outline["title"] = line[2:].strip()
            continue

        # Section Title
        if line.startswith('## '):
            section_title = line[3:].strip()
            current_section = {
                "title": section_title,
                "id": section_title.split('.')[0],
                "subsections": []
            }
            outline["sections"].append(current_section)
            continue
            
        # Subsection Title
        if line.startswith('### '):
            if current_section is None:
                # Handle case where a subsection appears before a section
                continue
            
            subsection_title = line[4:].strip()
            subsection_data = {
                "title": subsection_title,
                "id": subsection_title.split(' ')[0],
                "research_focus": "",
                "keywords": [],
                "table_recommended": False,  # é»˜è®¤ä¸å»ºè®®è¡¨æ ¼
                "table_number": None  # é»˜è®¤æ— è¡¨æ ¼ç¼–å·
            }
            current_section["subsections"].append(subsection_data)
            continue
            
        # Focus Line
        focus_match = focus_regex.match(line)
        if focus_match:
            if current_section and current_section["subsections"]:
                current_section["subsections"][-1]["research_focus"] = focus_match.group(1).strip()
            continue

        # Keywords Line
        keywords_match = keywords_regex.match(line)
        if keywords_match:
            if current_section and current_section["subsections"]:
                keywords_str = keywords_match.group(1).strip()
                keywords_list = [k.strip() for k in keywords_str.split(',')]
                current_section["subsections"][-1]["keywords"] = keywords_list
            continue
        
        # Table Recommended Line
        table_match = table_regex.match(line)
        if table_match:
            if current_section and current_section["subsections"]:
                table_value = table_match.group(1).strip().upper()
                current_section["subsections"][-1]["table_recommended"] = (table_value == "YES")
            continue
        
        # Table Number Line
        table_number_match = table_number_regex.match(line)
        if table_number_match:
            if current_section and current_section["subsections"]:
                table_num_str = table_number_match.group(1).strip()
                # å¦‚æœæ˜¯ N/A æˆ–ç©ºï¼Œåˆ™ä¸º Noneï¼›å¦åˆ™å°è¯•è½¬æ¢ä¸ºæ•´æ•°
                if table_num_str.upper() != "N/A" and table_num_str:
                    try:
                        current_section["subsections"][-1]["table_number"] = int(table_num_str)
                    except ValueError:
                        current_section["subsections"][-1]["table_number"] = None
            continue
            
    return outline


class DeepResearchAgent:
    def __init__(self, progress_callback=None, clarification_callback=None, chat_callback=None):
        self.table_counter = 0  # å…¨å±€è¡¨æ ¼è®¡æ•°å™¨
        self.progress_callback = progress_callback
        self.clarification_callback = clarification_callback
        self.chat_callback = chat_callback  # ç”¨äºå‘é€èŠå¤©æ¶ˆæ¯åˆ°å·¦ä¾§å¯¹è¯æ¡†
        self.clarification_queue = None  # æ¾„æ¸…ç­”æ¡ˆé˜Ÿåˆ—
        self.report_path = None  # æœ€ç»ˆæŠ¥å‘Šè·¯å¾„
    
    def _log(self, message: str):
        """æ›¿ä»£ printï¼Œé€šè¿‡å›è°ƒå‘é€åˆ°åç«¯ï¼ˆå³ä¾§é¢æ¿ï¼‰"""
        if self.progress_callback:
            asyncio.create_task(self.progress_callback({
                "type": "progress",
                "message": message
            }))
        else:
            print(message)  # é™çº§åˆ°æ™®é€š print
    
    def _log_chat(self, message: str):
        """å‘é€æ¶ˆæ¯åˆ°å·¦ä¾§å¯¹è¯æ¡†"""
        if self.chat_callback:
            asyncio.create_task(self.chat_callback({
                "type": "chat",
                "message": message
            }))
        else:
            print(message)  # é™çº§åˆ°æ™®é€š print
    
    async def wait_for_clarification(self):
        """ç­‰å¾…æ¾„æ¸…ç­”æ¡ˆ"""
        if self.clarification_queue:
            return await self.clarification_queue.get()
        return ""
    
    def set_clarification_queue(self, queue):
        """è®¾ç½®æ¾„æ¸…ç­”æ¡ˆé˜Ÿåˆ—"""
        self.clarification_queue = queue
        
    async def build(self):
        # 1. æ¾„æ¸… Agent
        self.clarifier_agent = SimpleAgent(
            name="ClarifierAgent",
            instructions=PROMPTS["clarifier"],
        )
        
        # 2. è§„åˆ’ Agent
        self.planner_agent = SimpleAgent(
            name="PlannerAgent",
            instructions=PROMPTS["planner_new"], # éœ€è¦ä¸€ä¸ªæ–°çš„ Promptï¼ŒæŒ‡å¯¼å®ƒåªç”Ÿæˆå¤§çº²JSON
  
        )
        
        self.searcher_agent = SimpleAgent(
            name="SearcherAgent",
            instructions=PROMPTS["searcher_cn"], # æˆ‘ä»¬å°†åˆ›å»ºè¿™ä¸ªæ–° Prompt
            tools=SEARCH_TOOLKIT.get_tools_in_agents() + SERPER_TOOLKIT.get_tools_in_agents()
        )
        
        # 3. ç« èŠ‚æ’°å†™ Agent (åŸ Formatterï¼Œä½† Prompt èšç„¦äºå•ç« èŠ‚å†™ä½œ)
        self.section_writer_agent = SimpleAgent(
            name="SectionWriterAgent",
            instructions=PROMPTS["section_writer"], # æ–° Promptï¼ŒæŒ‡å¯¼å®ƒæ ¹æ®èµ„æ–™æ’°å†™ç‰¹å®šç« èŠ‚
        )



    async def _clarify_task(self, task: str) -> str:
        """Helper function to handle the clarification step."""
        async with self.clarifier_agent as clarifier:
            clarification_result = clarifier.run_streamed(task)
            # æ¾„æ¸…é˜¶æ®µçš„æµå¼è¾“å‡ºå‘é€åˆ°å·¦ä¾§å¯¹è¯æ¡†
            await AgentsUtils.print_stream_events(
                    clarification_result.stream_events(),
                    callback=lambda text: self._log_chat(text) if text.strip() else None
                )
            clarification_questions = clarification_result.final_output

        if "æ— éœ€æ¾„æ¸…" not in clarification_questions:
            # é€šè¿‡å›è°ƒé€šçŸ¥åç«¯éœ€è¦æ¾„æ¸…
            if self.clarification_callback:
                await self.clarification_callback({
                    "type": "clarification_needed",
                    "questions": clarification_questions
                })
            
            # ç­‰å¾…æ¾„æ¸…ç­”æ¡ˆ
            user_clarifications = await self.wait_for_clarification()
            
            if user_clarifications.strip():
                # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ promptï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                merge_prompt = PROMPTS["clarifier_merge"].format(task=task, clarification_questions=clarification_questions, user_clarifications=user_clarifications)
                # å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„Agentæˆ–LLMè°ƒç”¨æ¥å®Œæˆæ•´åˆ
                # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥æ‹¼æ¥
                enhanced_task = merge_prompt
            else:
                enhanced_task = task
        else:
            enhanced_task = task
            # self._log("âœ… ä»»åŠ¡æ¸…æ™°ï¼Œæ— éœ€æ¾„æ¸…ã€‚")
        
        return enhanced_task

    def process_draft(self, draft_content: str, metadata: dict) -> str:
        """
        å¤„ç†åˆç¨¿ï¼Œæ’å…¥æ–‡çŒ®å¼•ç”¨
        
        Args:
            draft_content: åˆç¨¿å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
            metadata: å‚è€ƒæ–‡çŒ®å…ƒæ•°æ®å­—å…¸
            
        Returns:
            å¤„ç†åçš„å®Œæ•´æ–‡æ¡£ï¼ˆåŒ…å«å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼‰
        """
        processor = CitationProcessor(metadata)
        processed_content, reference_list = processor.process(draft_content)
        
        # ç»„è£…æœ€ç»ˆæ–‡æ¡£
        final_document = f"{processed_content}\n\n## å‚è€ƒæ–‡çŒ® (References)\n\n{reference_list}"
        
        return final_document
    
    def renumber_tables(self, content: str) -> str:
        """
        é‡æ–°æ•´ç†è¡¨æ ¼ç¼–å·ï¼Œç¡®ä¿è¡¨æ ¼ç¼–å·è¿ç»­ï¼ˆè¡¨1ã€è¡¨2ã€è¡¨3...ï¼‰
        
        Args:
            content: åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£å†…å®¹
            
        Returns:
            é‡æ–°ç¼–å·åçš„æ–‡æ¡£å†…å®¹
        """
        import re
        
        # æ‰¾å‡ºæ‰€æœ‰è¡¨æ ¼æ ‡é¢˜ï¼ˆæ ¼å¼ï¼š**è¡¨ X. æ ‡é¢˜**ï¼‰
        table_pattern = r'\*\*è¡¨\s+(\d+)\.\s+([^\*]+)\*\*'
        tables = list(re.finditer(table_pattern, content))
        
        if not tables:
            return content  # æ²¡æœ‰è¡¨æ ¼ï¼Œç›´æ¥è¿”å›
        
        # åˆ›å»ºæ—§ç¼–å·åˆ°æ–°ç¼–å·çš„æ˜ å°„
        old_to_new = {}
        for new_num, match in enumerate(tables, 1):
            old_num = match.group(1)
            old_to_new[old_num] = str(new_num)
        
        # æ›¿æ¢æ‰€æœ‰è¡¨æ ¼æ ‡é¢˜
        def replace_table_title(match):
            old_num = match.group(1)
            title = match.group(2)
            new_num = old_to_new[old_num]
            return f"**è¡¨ {new_num}. {title}**"
        
        content = re.sub(table_pattern, replace_table_title, content)
        
        
        return content

    async def run_streamed(self, task: str):
        """
        Orchestrates the entire research and writing process from planning to final report.
        """
        # === æ­¥éª¤ 0: åˆå§‹åŒ–å·¥ä½œåŒº ===
        project_id = f"research_{uuid.uuid4().hex[:8]}"
        workspace_dir = pathlib.Path(__file__).parent / "workspace" / project_id
        workspace_dir.mkdir(parents=True, exist_ok=True)
        # self._log(f"ğŸš€ åˆå§‹åŒ–æˆåŠŸï¼ä¸ºæœ¬æ¬¡ç ”ç©¶åˆ›å»ºä¸“å±å·¥ä½œåŒº: {workspace_dir}")

        try:
            # === æ­¥éª¤ 1: ä»»åŠ¡æ¾„æ¸… ===
            enhanced_task = await self._clarify_task(task)

            # === æ­¥éª¤ 2: ç”Ÿæˆç ”ç©¶å¤§çº² ===
            self._log("æ­£åœ¨æ’°å†™ç ”ç©¶æŠ¥å‘Š")
            async with self.planner_agent as planner:
                plan_result = planner.run_streamed(enhanced_task)
                # ä½¿ç”¨ callback å°†æµå¼è¾“å‡ºå‘é€åˆ°å‰ç«¯
                await AgentsUtils.print_stream_events(
                    plan_result.stream_events(),
                    callback=lambda text: self._log(text) if text else None
                )
                # è·å–æœ€ç»ˆè¾“å‡ºï¼ˆå¦‚æœæµå¼è¾“å‡ºæ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æœ€ç»ˆè¾“å‡ºï¼‰
                markdown_outline = plan_result.final_output
            
            # å°†å®Œæ•´å¤§çº²å‘é€åˆ°å‰ç«¯ï¼ˆç¡®ä¿å³ä¾§æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼‰
            # self._log(f"\n{markdown_outline}\n")
            
            # æŒä¹…åŒ–å¤§çº²
            outline_path = workspace_dir / "outline.md"
            outline_path.write_text(markdown_outline, encoding='utf-8')

            parsed_outline = parse_markdown_outline(markdown_outline)
            
            # æŒä¹…åŒ–è§£æåçš„å¤§çº²JSONï¼Œä¾¿äºè°ƒè¯•
            parsed_outline_path = workspace_dir / "outline.json"
            with parsed_outline_path.open('w', encoding='utf-8') as f:
                json.dump(parsed_outline, f, indent=2, ensure_ascii=False)

            # === æ­¥éª¤ 3: è¿­ä»£å¼ç ”ç©¶ä¸æ’°å†™ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰ ===
            
            written_sections = []
            all_sources_metadata = {}

            self._log("\n\n# å¼€å§‹æ’°å†™æ‰€æœ‰å­ç« èŠ‚\n\n")
            # æ”¶é›†æ‰€æœ‰å­ç« èŠ‚ï¼Œå¹¶ä¸ºæ¯ä¸ªç« èŠ‚é¢„å…ˆåˆ†é…è¡¨æ ¼ç¼–å·
            all_subsections = []
            for section in parsed_outline.get("sections", []):
                for subsection in section.get("subsections", []):
                    subsection['sections_title'] = section['title']
                    subsection['sections_id'] = section['id']
                    
                    # ä¸ºæ¯ä¸ªç« èŠ‚é¢„å…ˆåˆ†é…ä¸€ä¸ªæ½œåœ¨çš„è¡¨æ ¼ç¼–å·ï¼ˆé¿å…å¹¶è¡Œå†²çªï¼‰
                    self.table_counter += 1
                    subsection['assigned_table_number'] = self.table_counter
                    
                    all_subsections.append(subsection)
            
            # å®šä¹‰å•ä¸ªå­ç« èŠ‚çš„å¤„ç†å‡½æ•°
            async def process_subsection(subsection, semaphore):
                async with semaphore:
                    sub_id = subsection['id'].replace('.', '_')
                    self._log(f"\nå¼€å§‹å¤„ç†ç« èŠ‚ {subsection['title']}")
                    
                    try:
                        # --- 3a. æ‰§è¡Œæ·±åº¦æœç´¢ ---
                        self._log(f"\n[{subsection['id']}]æœç´¢ä¸­...")
                        searcher_prompt = f"""
                        ### **ç ”ç©¶ç„¦ç‚¹ (Research Focus)**:
                        {subsection['research_focus']}
                        
                        ### **å…³é”®è¯ (Keywords)**:
                        {", ".join(subsection['keywords'])}
                        """
                        
                        async with self.searcher_agent as searcher:
                            # å¢åŠ  max_turns é¿å…å¤æ‚æœç´¢æ—¶è¶…é™
                            result_stream = searcher.run_streamed(searcher_prompt)
                            
                            # æ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œè¿æ¥é”™è¯¯
                            max_retries = 3
                            retry_count = 0
                            markdown_data = None
                            
                            while retry_count < max_retries:
                                try:
                                    await AgentsUtils.print_stream_events(result_stream.stream_events())
                                    markdown_data = result_stream.final_output
                                    break
                                except (httpx.RemoteProtocolError, httpcore.RemoteProtocolError) as e:
                                    retry_count += 1
                                    self._log(f"    âš ï¸ ç½‘ç»œè¿æ¥é”™è¯¯ (å°è¯• {retry_count}/{max_retries}): {str(e)}")
                                    if retry_count < max_retries:
                                        await asyncio.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                                        # é‡æ–°åˆ›å»ºæµå¼è¿æ¥
                                        result_stream = searcher.run_streamed(searcher_prompt)
                                    else:
                                        self._log(f"    âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè·³è¿‡æ­¤ç« èŠ‚: {subsection['id']}")
                                        markdown_data = f"ã€ç½‘ç»œè¿æ¥é”™è¯¯ã€‘å¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"
                                        break
                                except Exception as e:
                                    self._log(f"    âŒ å…¶ä»–é”™è¯¯: {str(e)}")
                                    markdown_data = f"ã€å¤„ç†é”™è¯¯ã€‘å¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"
                                    break
                        
                        (workspace_dir / "data").mkdir(exist_ok=True)
                        search_data_path = workspace_dir / "data" / f"{sub_id}_raw_data.md"
                        search_data_path.write_text(markdown_data, encoding='utf-8')
                        # self._log(f"    ğŸ’¾ [{subsection['id']}] åŸå§‹æœç´¢æ•°æ®å·²ä¿å­˜: {search_data_path}")
                        
                        # --- 3b. è§£ææœç´¢ç»“æœ ---
                        # self._log(f"  ğŸ§© [{subsection['id']}] (3b) è§£ææœç´¢ç»“æœ...")
                        
                        raw_posts = [p.strip() for p in re.split(r'----', markdown_data) if p.strip()]
                        context_for_writer = ""
                        
                        if not raw_posts:
                            # self._log(f"    âš ï¸ [{subsection['id']}] è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æ–‡çŒ®ã€‚å°†ä¸ºæ­¤ç« èŠ‚ç”Ÿæˆå ä½å†…å®¹ã€‚")
                            return {
                                'sections_id': subsection['sections_id'],
                                'sections_title': subsection['sections_title'],
                                "id": subsection['id'], 
                                "title": subsection['title'], 
                                "content": "ã€ç¼–è€…æ³¨ï¼šæœªèƒ½æ‰¾åˆ°ç›¸å…³å‚è€ƒæ–‡çŒ®ï¼Œæ­¤ç« èŠ‚å†…å®¹ç¼ºå¤±ã€‚ã€‘",
                                "order": subsection['id']
                            }
                        
                        count = 0
                        for post_str in raw_posts:
                            try:
                                post_str = post_str.replace('----', '')
                                post = parse_document(post_str)
                                
                                # æ£€æŸ¥è§£æç»“æœæ˜¯å¦ä¸ºå­—å…¸
                                if not isinstance(post, dict):
                                    # self._log(f"[{subsection['id']}] è­¦å‘Š: è§£æç»“æœä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡")
                                    continue
                                
                                # æ£€æŸ¥æ˜¯å¦æœ‰ citation_key
                                if (ck := post.get('citation_key')):
                                    all_sources_metadata[ck] = post
                                    context_for_writer += f"### æ–‡çŒ®æ¥æº: {ck}\n**æ ‡é¢˜**: {post.get('title', 'N/A')}\n\n{post.get('content', 'N/A')}\n\n"
                                    count += 1
                            except Exception as e:
                                # self._log(f"[{subsection['id']}] è§£ææŸç¯‡æ–‡çŒ®å¤±è´¥: {str(e)}ï¼Œå·²è·³è¿‡ã€‚")
                                traceback.print_exc()
                        
                        # self._log(f"[{subsection['id']}] è§£æå®Œæˆï¼Œå¤„ç†äº† {count} ç¯‡æ–‡çŒ®ã€‚")
                        
                        # --- 3c. æ’°å†™ç« èŠ‚å†…å®¹ ---
                        # è·å– planner çš„è¡¨æ ¼å»ºè®®å’Œç¼–å·
                        table_recommended = subsection.get('table_recommended', False)
                        planner_table_number = subsection.get('table_number', None)
                        
                        # ä½¿ç”¨ planner å»ºè®®çš„ç¼–å·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é¢„åˆ†é…çš„ç¼–å·
                        assigned_table_number = planner_table_number if planner_table_number else subsection['assigned_table_number']
                        
                        # table_status = f"âœ… å»ºè®®è¡¨æ ¼{assigned_table_number}" if table_recommended else "ğŸ“ æ–‡å­—ä¸ºä¸»"
                        # self._log(f"  âœï¸  [{subsection['id']}] (3c) æ’°å†™ç« èŠ‚å†…å®¹... ({table_status})")
                        table_instruction = ""
                        
                        if table_recommended:
                            table_instruction = f"""
---
**ğŸ“Š è¡¨æ ¼å»ºè®®**ï¼šPlanner å»ºè®®æœ¬ç« èŠ‚ä½¿ç”¨è¡¨æ ¼æ¥å±•ç¤ºå¯¹æ¯”æˆ–æ±‡æ€»ä¿¡æ¯ã€‚
- **è¡¨æ ¼ç¼–å·**ï¼š**{assigned_table_number}**
- **é‡è¦**ï¼šç”Ÿæˆè¡¨æ ¼æ—¶ï¼Œå¿…é¡»åœ¨æ­£æ–‡æ®µè½ä¸­å¼•ç”¨è¡¨æ ¼ï¼Œä½¿ç”¨"å¦‚è¡¨{assigned_table_number}æ‰€ç¤º"ã€"è¯¦è§è¡¨{assigned_table_number}"ç­‰è¡¨è¿°ã€‚
- è¡¨æ ¼åº”æ”¾åœ¨æ­£æ–‡æœ«å°¾ï¼Œä½œä¸ºå†…å®¹çš„æ€»ç»“å’Œè¡¥å……ã€‚
"""
                        else:
                            table_instruction = f"""
---
**ğŸ“ å†™ä½œå»ºè®®**ï¼šæœ¬ç« èŠ‚ä¸»è¦ä½¿ç”¨æ–‡å­—è®ºè¿°å³å¯ï¼Œé€šå¸¸ä¸éœ€è¦è¡¨æ ¼ã€‚
é™¤éé‡åˆ°ç‰¹åˆ«é€‚åˆè¡¨æ ¼å±•ç¤ºçš„å¯†é›†æ•°æ®ï¼Œå¦åˆ™è¯·ç”¨æ¸…æ™°çš„æ–‡å­—è¡¨è¿°å†…å®¹ã€‚
å¦‚æœç¡®å®éœ€è¦è¡¨æ ¼ï¼Œå¯ä½¿ç”¨ç¼–å·ï¼š**{assigned_table_number}**ï¼Œå¹¶åœ¨æ­£æ–‡ä¸­å¼•ç”¨ã€‚
"""
                        
                        writer_prompt = f"""
### **ç« èŠ‚æ ‡é¢˜**: {subsection['title']}

### **ç« èŠ‚ç„¦ç‚¹**: {subsection['research_focus']}

### **ç›¸å…³ç ”ç©¶èµ„æ–™**:
{context_for_writer}
{table_instruction}
"""
                        

                        async with self.section_writer_agent as writer:
                            section_result = writer.run_streamed(writer_prompt)
                            await AgentsUtils.print_stream_events(section_result.stream_events())
                            written_content = section_result.final_output
                            
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                        # if f"**è¡¨ {assigned_table_number}." in written_content:
                        #     self._log(f"    ğŸ“Š [{subsection['id']}] å·²ç”Ÿæˆè¡¨æ ¼ï¼šè¡¨ {assigned_table_number}")
                        
                        (workspace_dir / "drafts").mkdir(exist_ok=True)
                        draft_path = workspace_dir / "drafts" / f"{sub_id}_draft.md"
                        draft_path.write_text(written_content, encoding='utf-8')
                        # self._log(f"    ğŸ’¾ [{subsection['id']}] ç« èŠ‚è‰ç¨¿å·²ä¿å­˜: {draft_path}")
                        
                        self._log(f"\n\n[{subsection['id']}] ç« èŠ‚æ’°å†™å®Œæˆï¼")
                        
                        # å°†ç« èŠ‚å†…å®¹å‘é€åˆ°å‰ç«¯å³ä¾§é¢æ¿æ˜¾ç¤º
                        # self._log(f"\n### {subsection['title']}\n{written_content}\n")
                        
                        return {
                            'sections_id': subsection['sections_id'],
                            'sections_title': subsection['sections_title'],
                            "id": subsection['id'], 
                            "title": subsection['title'], 
                            "content": written_content,
                            "order": subsection['id']
                        }
                    
                    except Exception as e:
                        # self._log(f"âŒ [{subsection['id']}] å¤„ç†å‡ºé”™: {e}")
                        traceback.print_exc()
                        return {
                            'sections_id': subsection['sections_id'],
                            'sections_title': subsection['sections_title'],
                            "id": subsection['id'], 
                            "title": subsection['title'], 
                            "content": f"ã€ç¼–è€…æ³¨ï¼šå¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}ã€‘",
                            "order": subsection['id']
                        }
            
            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(CONCURRENCY)
            
            # å¹¶è¡Œå¤„ç†æ‰€æœ‰å­ç« èŠ‚
            tasks = [process_subsection(subsection, semaphore) for subsection in all_subsections]
            results = await asyncio.gather(*tasks)
            
            # æŒ‰ç…§åŸå§‹é¡ºåºæ’åºç»“æœ
            written_sections = sorted(results, key=lambda x: x['order'])
            

            # === æ­¥éª¤ 4: æœ€ç»ˆæ•´åˆä¸å®¡é˜… ===
            # self._log("\næ­£åœ¨è¿›è¡Œæœ€ç»ˆæ•´åˆä¸å®¡é˜…...")

            # å‡†å¤‡æœ€ç»ˆ prompt æ‰€éœ€çš„ææ–™
            title = parsed_outline.get("title", "æœªå‘½åç»¼è¿°")
            full_content = f"# {title}\n\n"
            
            # å…ˆæ‰¾å‡ºæœ€åä¸€ä¸ªç« èŠ‚çš„ID
            last_section_id = None
            if written_sections:
                last_section_id = written_sections[-1]['sections_id']
            
            current_section_id = None # ç”¨äºè¿½è¸ªå½“å‰çš„ç« èŠ‚ID

            for section in written_sections:
                # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº†ä¸€ä¸ªæ–°çš„å¤§ç« èŠ‚
                if section['sections_id'] != current_section_id:
                    full_content += f"# {section['sections_title']}\n\n"
                    current_section_id = section['sections_id'] # æ›´æ–°å½“å‰ç« èŠ‚ID

                # ç¬¬ä¸€ç« å’Œæœ€åä¸€ç« ä¸æ·»åŠ å°æ ‡é¢˜ï¼Œå…¶ä»–ç« èŠ‚æ·»åŠ å°æ ‡é¢˜
                if current_section_id == '1' or current_section_id == last_section_id:
                    full_content += f"{section['content']}\n\n"
                else:
                    full_content += f"### {section['title']}\n{section['content']}\n\n"
        
            all_sources_metadata_json = json.dumps(all_sources_metadata, indent=2, ensure_ascii=False)
            
            # æŒä¹…åŒ–æ•´åˆå‰çš„ææ–™
            (workspace_dir / "final_inputs").mkdir(exist_ok=True)
            (workspace_dir / "final_inputs" / "full_content.md").write_text(full_content, encoding='utf-8')
            (workspace_dir / "final_inputs" / "all_metadata.json").write_text(all_sources_metadata_json, encoding='utf-8')

            # å¤„ç†æ–‡çŒ®å¼•ç”¨
            final_output = self.process_draft(full_content, all_sources_metadata)
            
            # é‡æ–°æ•´ç†è¡¨æ ¼ç¼–å·ï¼Œç¡®ä¿è¿ç»­æ€§
            final_output = self.renumber_tables(final_output)
            
            # æŒä¹…åŒ–æœ€ç»ˆæŠ¥å‘Š
            final_report_path = workspace_dir / "final_report.md"
            final_report_path.write_text(final_output, encoding='utf-8')
            # self._log(f"  ğŸ’¾ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {final_report_path}")
            
            # ä¿å­˜æŠ¥å‘Šè·¯å¾„åˆ°å®ä¾‹å±æ€§ï¼Œä¾› API æœåŠ¡å™¨ä½¿ç”¨
            self.report_path = str(final_report_path.absolute())

            # å‘é€æŠ¥å‘Šå®Œæˆé€šçŸ¥ï¼ŒåŒ…å«å®Œæ•´çš„æŠ¥å‘Šå†…å®¹
            if self.progress_callback:
                print(f"[DEBUG] å‡†å¤‡å‘é€ report_completedï¼Œfinal_output é•¿åº¦: {len(final_output) if final_output else 0}")
                print(f"[DEBUG] final_output é¢„è§ˆ: {final_output[:200] if final_output else 'None'}...")
                await self.progress_callback({
                    "type": "report_completed",
                    "message": "ç»¼è¿°æ’°å†™å®Œæˆï¼",
                    "report_content": final_output
                })
            
            return final_output

        except Exception as e:
            # self._log(f"\nâŒ åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            traceback.print_exc()
            return f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ã€‚è¯·æ£€æŸ¥å·¥ä½œåŒºæ–‡ä»¶å¤¹ '{workspace_dir}' ä¸­çš„æ—¥å¿—å’Œä¸­é—´æ–‡ä»¶ä»¥è¿›è¡Œè°ƒè¯•ã€‚"


async def main():
    deep_research = DeepResearchAgent()
    await deep_research.build()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨ Web ç¯å¢ƒä¸­è¿è¡Œ
    import sys
    if hasattr(sys, 'ps1') or sys.stdin.isatty():
        # äº¤äº’å¼ç¯å¢ƒï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ input()
        query = input("What would you like to research? ")
        if not query.strip():
            print("âŒ é”™è¯¯: è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜")
            return
    else:
        # Web ç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢æˆ–ä»ç¯å¢ƒå˜é‡è·å–
        query = "STAT6 åœ¨ç‰¹åº”æ€§çš®ç‚ä¸­çš„ç ”ç©¶è¿›å±•"  # é»˜è®¤æŸ¥è¯¢
        print(f"Web ç¯å¢ƒä½¿ç”¨é»˜è®¤æŸ¥è¯¢: {query}")
    
    print(f"Processing task: {query}")
    result = await deep_research.run_streamed(query)

    with open("final_report.md", "w") as f:
        f.write(result)
    print(f"{'-' * 80}\n{result}\n{'-' * 80}")


if __name__ == "__main__":
    asyncio.run(main())
