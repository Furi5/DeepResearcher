# pylint: disable=line-too-long
# ruff: noqa: E501
"""
https://manus.im/blog/introducing-wide-research
"""

import asyncio
import json
import pathlib
import traceback
import re
import uuid
import yaml
import httpx
import httpcore

from agents import function_tool

from utu.agents import SimpleAgent
from utu.config import ConfigLoader
from utu.tools import SearchToolkit, SerperToolkit, SerpApiToolkit
from utu.utils import AgentsUtils, FileUtils, WebhookManager
from utu.utils.Citation import CitationProcessor
from utu.utils.text_process import parse_document, parse_markdown_outline
from utu.utils.text_process import process_draft, renumber_tables

PROMPTS = FileUtils.load_yaml(pathlib.Path(__file__).parent / "prompts.yaml")
CONCURRENCY = 20



class DeepResearchAgent:
    def __init__(
        self, 
        webhook_url: str | None = None, 
        task_id: str | None = None,
        use_fixed_workspace: bool = False
    ):
        self.table_counter = 0  # å…¨å±€è¡¨æ ¼è®¡æ•°å™¨
        self.clarification_queue = None  # æ¾„æ¸…ç­”æ¡ˆé˜Ÿåˆ—
        self.report_path = None  # æœ€ç»ˆæŠ¥å‘Šè·¯å¾„
        self.progress_callback = None  # è¿›åº¦å›è°ƒå‡½æ•°
        self.use_fixed_workspace = use_fixed_workspace  # æ˜¯å¦ä½¿ç”¨å›ºå®šå·¥ä½œåŒº
        
        # åˆå§‹åŒ– webhook ç®¡ç†å™¨
        self.webhook_manager = WebhookManager(webhook_url, task_id)
        
        # åˆå§‹åŒ–å·¥å…·ï¼ˆå¸¦ webhookï¼‰
        self.search_toolkit = SearchToolkit(
            ConfigLoader.load_toolkit_config("search"),
            webhook_manager=self.webhook_manager
        )
        # ä½¿ç”¨ SerpApiï¼ˆæ”¯æŒå¤šæœç´¢å¼•æ“ï¼‰
        self.serpapi_toolkit = SerpApiToolkit(
            ConfigLoader.load_toolkit_config("serpapi"),
            webhook_manager=self.webhook_manager
        )
        # SerperToolkit æš‚æ—¶ç¦ç”¨ï¼ˆAPI Key æ— æ•ˆï¼‰
        # self.serper_toolkit = SerperToolkit(
        #     ConfigLoader.load_toolkit_config("serper"),
        #     webhook_manager=self.webhook_manager
        # )
        
        # è¿›åº¦è¿½è¸ª
        self.progress_counter = 0  # å½“å‰å®Œæˆçš„æ­¥éª¤æ•°
        self.total_steps = 0  # æ€»æ­¥éª¤æ•°ï¼ˆå­ç« èŠ‚æ•° Ã— 2ï¼‰
        self.progress_lock = asyncio.Lock()  # ç”¨äºçº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°
    
    async def build(self):
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„callbackå¤„ç†å™¨
        class SimpleCallbackHandler:
            def __init__(self, webhook_manager):
                self.webhook_manager = webhook_manager

            async def send_step_update(self, data):
                """å‘é€æ­¥éª¤æ›´æ–°"""
                if data.get("search_type") == "tool_searching":
                    # å‘é€å·¥å…·æœç´¢çš„è¯¦ç»† callback
                    await self.webhook_manager.send_callback(
                        status="running",
                        results_type="tool_search",
                        step={
                            "name": "ToolSearching",
                            "description": "å·¥å…·æœç´¢",
                            "message": f"æ­£åœ¨åˆ†æå’ŒæŸ¥è¯¢{data.get('search_query', '')}ç›¸å…³ä¿¡æ¯..."
                        },
                        data=[{
                            "tool_name": data.get('tool_name', ''),
                            "search_query": data.get('search_query', ''),
                            "found_urls": data.get('found_urls', []),
                            "url_count": data.get('url_count', 0)
                        }]
                    )
                    
                    # åŒæ—¶å‘é€æœç´¢ç»“æœæ›´æ–°ï¼ˆç”¨äºç™½åå•è¿‡æ»¤ï¼‰
                    if data.get("found_urls"):
                        await self.webhook_manager.send_search_result(
                            data.get("search_query", ""),
                            [{"url": url, "title": f"Search result {i+1}", "snippet": f"From {data.get('tool_name', 'unknown')}"} 
                             for i, url in enumerate(data.get("found_urls", []))]
                        )

        callback = SimpleCallbackHandler(self.webhook_manager)

        self.planner_agent = SimpleAgent(
            name="PlannerAgent",
            instructions=PROMPTS["planner_new"], # éœ€è¦ä¸€ä¸ªæ–°çš„ Promptï¼ŒæŒ‡å¯¼å®ƒåªç”Ÿæˆå¤§çº²JSON

        )

        # åˆ›å»ºå·¥å…· callback æ”¯æŒ
        async def tool_callback(data):
            """å·¥å…·å›è°ƒå‡½æ•°"""
            await callback.send_step_update(data)

        def wrap_tool_with_callback(original_func, callback_func):
            """åŒ…è£…å·¥å…·å‡½æ•°ä»¥æ·»åŠ  callback æ”¯æŒ"""
            async def wrapped_tool(*args, **kwargs):
                # å¦‚æœåŸå‡½æ•°æ”¯æŒ callback å‚æ•°ï¼Œåˆ™æ·»åŠ 
                import inspect
                sig = inspect.signature(original_func)
                if 'callback' in sig.parameters:
                    kwargs['callback'] = callback_func
                return await original_func(*args, **kwargs)
            
            # ä¿ç•™åŸå‡½æ•°çš„å…ƒæ•°æ®ï¼ˆå‡½æ•°åã€æ–‡æ¡£å­—ç¬¦ä¸²ç­‰ï¼‰
            wrapped_tool.__name__ = original_func.__name__
            wrapped_tool.__doc__ = original_func.__doc__
            wrapped_tool.__annotations__ = original_func.__annotations__
            
            return wrapped_tool

        # è·å–åŸå§‹å·¥å…·å‡½æ•°å¹¶åŒ…è£…
        search_tools = []
        
        # å¤„ç† SearchToolkit çš„å·¥å…·
        for tool_name, tool_func in self.search_toolkit.get_tools_map_func().items():
            wrapped_func = wrap_tool_with_callback(tool_func, tool_callback)
            search_tools.append(function_tool(wrapped_func, strict_mode=False))
        
        # å¤„ç† SerpApiToolkit çš„å·¥å…·
        for tool_name, tool_func in self.serpapi_toolkit.get_tools_map_func().items():
            wrapped_func = wrap_tool_with_callback(tool_func, tool_callback)
            search_tools.append(function_tool(wrapped_func, strict_mode=False))

        self.searcher_agent = SimpleAgent(
            name="SearcherAgent",
            instructions=PROMPTS["searcher_cn"],
            tools=search_tools
        )
        
        self.section_writer_agent = SimpleAgent(
            name="SectionWriterAgent",
            instructions=PROMPTS["section_writer"], # æ–° Promptï¼ŒæŒ‡å¯¼å®ƒæ ¹æ®èµ„æ–™æ’°å†™ç‰¹å®šç« èŠ‚
        )

    async def _initialize_workspace(self, task: str) -> pathlib.Path:
        """æ­¥éª¤ 0: åˆå§‹åŒ–å·¥ä½œåŒº"""
        if self.use_fixed_workspace:
            # ä½¿ç”¨å›ºå®šçš„å·¥ä½œåŒºï¼ˆç”¨äºæµ‹è¯•å’Œè°ƒè¯•ï¼‰
            project_id = "test_workspace"
        else:
            # æ¯æ¬¡åˆ›å»ºæ–°çš„å·¥ä½œåŒºï¼ˆç”¨äºç”Ÿäº§ï¼‰
            project_id = f"research_{uuid.uuid4().hex[:8]}"
        
        workspace_dir = pathlib.Path(__file__).parent / "workspace" / project_id
        workspace_dir.mkdir(parents=True, exist_ok=True)
        return workspace_dir

    async def _generate_outline(self, task: str) -> dict:
        """æ­¥éª¤ 1: ç”Ÿæˆç ”ç©¶å¤§çº²"""
        async with self.planner_agent as planner:
            plan_result = await planner.run(task)
            markdown_outline = plan_result.get_run_result().final_output

        # æŒä¹…åŒ–å¤§çº²
        outline_path = workspace_dir / "outline.md"
        outline_path.write_text(markdown_outline, encoding='utf-8')

        parsed_outline = parse_markdown_outline(markdown_outline)

        return parsed_outline

    async def _prepare_subsections(self, parsed_outline: dict) -> list:
        """æ­¥éª¤ 2: å‡†å¤‡å­ç« èŠ‚æ•°æ®"""
        all_subsections = []
        for section in parsed_outline.get("sections", []):
            for subsection in section.get("subsections", []):
                subsection['sections_title'] = section['title']
                subsection['sections_id'] = section['id']

                # ä¸ºæ¯ä¸ªç« èŠ‚é¢„å…ˆåˆ†é…ä¸€ä¸ªæ½œåœ¨çš„è¡¨æ ¼ç¼–å·ï¼ˆé¿å…å¹¶è¡Œå†²çªï¼‰
                self.table_counter += 1
                subsection['assigned_table_number'] = self.table_counter

                all_subsections.append(subsection)
        
        # è®¾ç½®æ€»æ­¥éª¤æ•°ï¼šæ¯ä¸ªå­ç« èŠ‚éœ€è¦æœç´¢ + æ’°å†™ = 2 æ­¥
        self.total_steps = len(all_subsections) * 2
        
        return all_subsections

    async def _process_single_subsection(self, subsection: dict) -> tuple[dict, dict]:
        """å¤„ç†å•ä¸ªå­ç« èŠ‚çš„å®Œæ•´æµç¨‹ï¼Œè¿”å› (ç« èŠ‚ç»“æœ, æ¥æºå…ƒæ•°æ®)"""
        sub_id = subsection['id'].replace('.', '_')
        section_id = subsection['id']
        section_title = subsection['title']

        try:
            # --- 3a. æ‰§è¡Œæ·±åº¦æœç´¢ ---
            markdown_data = await self._perform_research_search(subsection, sub_id)

            # æœç´¢å®Œæˆï¼Œæ›´æ–°è¿›åº¦
            async with self.progress_lock:
                self.progress_counter += 1
                percentage = int((self.progress_counter / self.total_steps) * 100) if self.total_steps > 0 else 0
                

            # --- 3b. è§£ææœç´¢ç»“æœ ---
            context_for_writer, sources_metadata = await self._parse_search_results(
                markdown_data, subsection, sub_id) 

            # --- 3c. æ’°å†™ç« èŠ‚å†…å®¹ ---
            written_content = await self._write_section_content(subsection, context_for_writer, sub_id)

            # æ’°å†™å®Œæˆï¼Œæ›´æ–°è¿›åº¦
            async with self.progress_lock:
                self.progress_counter += 1
                percentage = int((self.progress_counter / self.total_steps) * 100) if self.total_steps > 0 else 0
                
                # print(f"âœ… [æ’°å†™å®Œæˆ] {section_id} {section_title} - è¿›åº¦: {self.progress_counter}/{self.total_steps} ({percentage}%)")
                await self.webhook_manager.send_section_completed(
                    section_id, section_title, self.progress_counter, self.total_steps
                )

            section_result = {
                'sections_id': subsection['sections_id'],
                'sections_title': subsection['sections_title'],
                "id": subsection['id'],
                "title": subsection['title'],
                "content": written_content,
                "order": subsection['id']
            }

            return section_result, sources_metadata

        except Exception as e:
            error_msg = str(e)
            # print(f"âŒ [é”™è¯¯] {section_id} {section_title} - {error_msg[:100]}")
            traceback.print_exc()
            
            # å¦‚æœæ˜¯ API é”™è¯¯ï¼Œå‘é€é”™è¯¯ webhook
            if "402" in error_msg or "Insufficient Balance" in error_msg:
                await self.webhook_manager.send_error(
                    "E5003", 
                    f"ç« èŠ‚ {section_id} å¤„ç†å¤±è´¥: API ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼"
                )
            elif "503" in error_msg or "502" in error_msg or "API" in error_msg:
                await self.webhook_manager.send_error(
                    "E5002", 
                    f"ç« èŠ‚ {section_id} å¤„ç†å¤±è´¥: API æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                )
            
            section_result = {
                'sections_id': subsection['sections_id'],
                'sections_title': subsection['sections_title'],
                "id": subsection['id'],
                "title": subsection['title'],
                "content": f"ã€ç¼–è€…æ³¨ï¼šå¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•ã€‘",
                "order": subsection['id']
            }
            return section_result, {}

    async def _perform_research_search(self, subsection: dict, sub_id: str) -> str:
        """æ‰§è¡Œæ·±åº¦æœç´¢"""
        searcher_prompt = f"""
        ### **ç ”ç©¶ç„¦ç‚¹ (Research Focus)**:
        {subsection['research_focus']}

        ### **å…³é”®è¯ (Keywords)**:
        {", ".join(subsection['keywords'])}
        """

        async with self.searcher_agent as searcher:
            # å¢åŠ  max_turns é¿å…å¤æ‚æœç´¢æ—¶è¶…é™
            result_stream = searcher.run_streamed(searcher_prompt)

            # æ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œè¿æ¥é”™è¯¯
            max_retries = 3
            retry_count = 0
            markdown_data = None

            while retry_count < max_retries:
                try:
                    await AgentsUtils.print_stream_events(result_stream.stream_events())
                    markdown_data = result_stream.final_output
                    break
                except (httpx.RemoteProtocolError, httpcore.RemoteProtocolError) as e:
                    retry_count += 1

                    if retry_count < max_retries:
                        await asyncio.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                        # é‡æ–°åˆ›å»ºæµå¼è¿æ¥
                        result_stream = searcher.run_streamed(searcher_prompt)
                    else:
                        markdown_data = f"ã€ç½‘ç»œè¿æ¥é”™è¯¯ã€‘å¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"
                        break
                except Exception as e:
                    markdown_data = f"ã€å¤„ç†é”™è¯¯ã€‘å¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"
                    break

        return markdown_data

    async def _parse_search_results(self, markdown_data: str, subsection: dict, sub_id: str) -> tuple[str, dict]:
        """è§£ææœç´¢ç»“æœ"""
        raw_posts = [p.strip() for p in re.split(r'----', markdown_data) if p.strip()]
        context_for_writer = ""
        all_sources_metadata = {}

        if not raw_posts:
            return context_for_writer, all_sources_metadata

        count = 0
        for post_str in raw_posts:
            try:
                post_str = post_str.replace('----', '')
                post = parse_document(post_str)

                # æ£€æŸ¥è§£æç»“æœæ˜¯å¦ä¸ºå­—å…¸
                if not isinstance(post, dict):
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰ citation_key
                if (ck := post.get('citation_key')):
                    all_sources_metadata[ck] = post
                    context_for_writer += f"### æ–‡çŒ®æ¥æº: {ck}\n**æ ‡é¢˜**: {post.get('title', 'N/A')}\n\n{post.get('content', 'N/A')}\n\n"
                    count += 1
                    # print(f"[DEBUG] Found citation_key: {ck}")
                else:
                    # print(f"[DEBUG] No citation_key in post: {post.keys() if post else 'None'}")
            except Exception as e:
                traceback.print_exc()

        return context_for_writer, all_sources_metadata

    async def _write_section_content(self, subsection: dict, context_for_writer: str,
                                   sub_id: str) -> str:
        """æ’°å†™ç« èŠ‚å†…å®¹"""
        # è·å– planner çš„è¡¨æ ¼å»ºè®®å’Œç¼–å·
        table_recommended = subsection.get('table_recommended', False)
        planner_table_number = subsection.get('table_number', None)

        # ä½¿ç”¨ planner å»ºè®®çš„ç¼–å·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é¢„åˆ†é…çš„ç¼–å·
        assigned_table_number = planner_table_number if planner_table_number else subsection['assigned_table_number']

        table_instruction = ""

        if table_recommended:
            table_instruction = f"""
---
**ğŸ“Š è¡¨æ ¼å»ºè®®**ï¼šPlanner å»ºè®®æœ¬ç« èŠ‚ä½¿ç”¨è¡¨æ ¼æ¥å±•ç¤ºå¯¹æ¯”æˆ–æ±‡æ€»ä¿¡æ¯ã€‚
- **è¡¨æ ¼ç¼–å·**ï¼š**{assigned_table_number}**
- **é‡è¦**ï¼šç”Ÿæˆè¡¨æ ¼æ—¶ï¼Œå¿…é¡»åœ¨æ­£æ–‡æ®µè½ä¸­å¼•ç”¨è¡¨æ ¼ï¼Œä½¿ç”¨"å¦‚è¡¨{assigned_table_number}æ‰€ç¤º"ã€"è¯¦è§è¡¨{assigned_table_number}"ç­‰è¡¨è¿°ã€‚
- è¡¨æ ¼åº”æ”¾åœ¨æ­£æ–‡æœ«å°¾ï¼Œä½œä¸ºå†…å®¹çš„æ€»ç»“å’Œè¡¥å……ã€‚
"""
        else:
            table_instruction = f"""
---
**ğŸ“ å†™ä½œå»ºè®®**ï¼šæœ¬ç« èŠ‚ä¸»è¦ä½¿ç”¨æ–‡å­—è®ºè¿°å³å¯ï¼Œé€šå¸¸ä¸éœ€è¦è¡¨æ ¼ã€‚
é™¤éé‡åˆ°ç‰¹åˆ«é€‚åˆè¡¨æ ¼å±•ç¤ºçš„å¯†é›†æ•°æ®ï¼Œå¦åˆ™è¯·ç”¨æ¸…æ™°çš„æ–‡å­—è¡¨è¿°å†…å®¹ã€‚
å¦‚æœç¡®å®éœ€è¦è¡¨æ ¼ï¼Œå¯ä½¿ç”¨ç¼–å·ï¼š**{assigned_table_number}**ï¼Œå¹¶åœ¨æ­£æ–‡ä¸­å¼•ç”¨ã€‚
"""

        writer_prompt = f"""
### **ç« èŠ‚æ ‡é¢˜**: {subsection['title']}

### **ç« èŠ‚ç„¦ç‚¹**: {subsection['research_focus']}

### **ç›¸å…³ç ”ç©¶èµ„æ–™**:
{context_for_writer}
{table_instruction}
"""

        async with self.section_writer_agent as writer:
            section_result = writer.run_streamed(writer_prompt)
            await AgentsUtils.print_stream_events(section_result.stream_events())
            written_content = section_result.final_output


        return written_content

    async def _process_subsections_parallel(self, all_subsections: list) -> tuple[list, dict]:
        """æ­¥éª¤ 3: å¹¶è¡Œå¤„ç†æ‰€æœ‰å­ç« èŠ‚"""
        written_sections = []
        all_sources_metadata = {}

        # å®šä¹‰å•ä¸ªå­ç« èŠ‚çš„å¤„ç†å‡½æ•°
        async def process_subsection(subsection, semaphore):
            async with semaphore:
                section_result, sources_metadata = await self._process_single_subsection(subsection)
                return section_result, sources_metadata

        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
        semaphore = asyncio.Semaphore(CONCURRENCY)

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å­ç« èŠ‚
        tasks = [process_subsection(subsection, semaphore) for subsection in all_subsections]
        results = await asyncio.gather(*tasks)

        # åˆ†ç¦»ç« èŠ‚ç»“æœå’Œå…ƒæ•°æ®ï¼Œå¹¶åˆå¹¶æ‰€æœ‰å…ƒæ•°æ®
        for section_result, sources_metadata in results:
            written_sections.append(section_result)
            all_sources_metadata.update(sources_metadata)

        # æŒ‰ç…§åŸå§‹é¡ºåºæ’åºç»“æœ
        written_sections = sorted(written_sections, key=lambda x: x['order'])

        return written_sections, all_sources_metadata

    async def _integrate_final_report(self, parsed_outline: dict, written_sections: list,
                                    all_sources_metadata: dict) -> str:
        """æ­¥éª¤ 4: æœ€ç»ˆæ•´åˆä¸å®¡é˜…"""
        # å‡†å¤‡æœ€ç»ˆ prompt æ‰€éœ€çš„ææ–™
        title = parsed_outline.get("title", "æœªå‘½åç»¼è¿°")
        full_content = f"# {title}\n\n"

        # å…ˆæ‰¾å‡ºæœ€åä¸€ä¸ªç« èŠ‚çš„ID
        last_section_id = None
        if written_sections:
            last_section_id = written_sections[-1]['sections_id']

        current_section_id = None # ç”¨äºè¿½è¸ªå½“å‰çš„ç« èŠ‚ID

        for section in written_sections:
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº†ä¸€ä¸ªæ–°çš„å¤§ç« èŠ‚
            if section['sections_id'] != current_section_id:
                full_content += f"# {section['sections_title']}\n\n"
                current_section_id = section['sections_id'] # æ›´æ–°å½“å‰ç« èŠ‚ID

            # ç¬¬ä¸€ç« å’Œæœ€åä¸€ç« ä¸æ·»åŠ å°æ ‡é¢˜ï¼Œå…¶ä»–ç« èŠ‚æ·»åŠ å°æ ‡é¢˜
            if current_section_id == '1' or current_section_id == last_section_id:
                full_content += f"{section['content']}\n\n"
            else:
                full_content += f"### {section['title']}\n{section['content']}\n\n"

        all_sources_metadata_json = json.dumps(all_sources_metadata, indent=2, ensure_ascii=False)


        # å¤„ç†æ–‡çŒ®å¼•ç”¨
        # print(f"[DEBUG] all_sources_metadata keys: {list(all_sources_metadata.keys()) if all_sources_metadata else 'Empty'}")
        final_output = process_draft(full_content, all_sources_metadata)

        # é‡æ–°æ•´ç†è¡¨æ ¼ç¼–å·ï¼Œç¡®ä¿è¿ç»­æ€§
        final_output = renumber_tables(final_output)

        # ä» final_output ä¸­è§£æå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        citations = self._parse_citations_from_report(final_output)
        
        await self.webhook_manager.send_final_result(final_output, citations)

        return final_output
    
    def _parse_citations_from_report(self, final_output: str) -> dict:
        """ä»æœ€ç»ˆæŠ¥å‘Šä¸­è§£æå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        
        Args:
            final_output: æœ€ç»ˆæŠ¥å‘Šå†…å®¹
            
        Returns:
            dict: {ç¼–å·: å‚è€ƒæ–‡çŒ®å†…å®¹}ï¼Œä¾‹å¦‚ {"1": "[1] Author. Year. Title...", "2": ...}
        """
        citations = {}
        
        # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
        ref_section_match = re.search(r'## å‚è€ƒæ–‡çŒ®.*?\n\n(.*)', final_output, re.DOTALL)
        if not ref_section_match:
            # print("[WARNING] æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†")
            return citations
        
        references_text = ref_section_match.group(1)
        
        # è§£ææ¯ä¸ªå‚è€ƒæ–‡çŒ®æ¡ç›®ï¼š[æ•°å­—] å†…å®¹
        pattern = r'\[(\d+)\]\s+(.+?)(?=\n\[|\n\n|\Z)'
        matches = re.findall(pattern, references_text, re.DOTALL)
        
        for num, content in matches:
            # æ¸…ç†å†…å®¹ï¼ˆå»é™¤å¤šä½™çš„æ¢è¡Œï¼‰
            content = content.strip().replace('\n', ' ')
            citations[num] = f"[{num}] {content}"
        
        # print(f"âœ… è§£æåˆ° {len(citations)} æ¡å‚è€ƒæ–‡çŒ®")
        return citations

    async def run_streamed(self, parsed_outline: dict):
        """
        Orchestrates the entire research and writing process from planning to final report.
        """
        try:
            # print("\n" + "="*80)
            # print("ğŸš€ å¼€å§‹ç ”ç©¶ä»»åŠ¡")
            # print("="*80)
            

            # === æ­¥éª¤ 2: å‡†å¤‡å­ç« èŠ‚æ•°æ® ===
            # print("\nğŸ“ æ­¥éª¤ 2: å‡†å¤‡å­ç« èŠ‚æ•°æ®...")
            all_subsections = await self._prepare_subsections(parsed_outline)
            # print(f"âœ… å‡†å¤‡å®Œæˆ: {len(all_subsections)} ä¸ªå­ç« èŠ‚")
            # print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {self.total_steps} (æœç´¢ + æ’°å†™)")
            # print("\n" + "="*80)
            # print("ğŸ” å¼€å§‹å¹¶è¡Œå¤„ç† (æœç´¢ + æ’°å†™)")
            # print("="*80)

            # === æ­¥éª¤ 3: è¿­ä»£å¼ç ”ç©¶ä¸æ’°å†™ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰ ===
            written_sections, all_sources_metadata = await self._process_subsections_parallel(
                all_subsections)
            
            # print("\n" + "="*80)
            # print("âœ… æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆ!")
            # print(f"ğŸ“š å®Œæˆç« èŠ‚æ•°: {len(written_sections)}")
            # print(f"ğŸ“– æ”¶é›†æ–‡çŒ®æ•°: {len(all_sources_metadata)}")
            # print("="*80)

            # === æ­¥éª¤ 4: æœ€ç»ˆæ•´åˆä¸å®¡é˜… ===
            # print("\nğŸ”„ æ­¥éª¤ 4: æ•´åˆæœ€ç»ˆæŠ¥å‘Š...")
            final_output = await self._integrate_final_report(
                parsed_outline, written_sections, all_sources_metadata)
            
            # print("\n" + "="*80)
            # print("ğŸ‰ ç ”ç©¶ä»»åŠ¡å®Œæˆ!")
            # print(f"ğŸ“„ æŠ¥å‘Šé•¿åº¦: {len(final_output)} å­—ç¬¦")
            # print("="*80)

            # å‘é€æŠ¥å‘Šå®Œæˆé€šçŸ¥ï¼ŒåŒ…å«å®Œæ•´çš„æŠ¥å‘Šå†…å®¹
            if self.progress_callback:
                # print(f"[DEBUG] å‡†å¤‡å‘é€ report_completedï¼Œfinal_output é•¿åº¦: {len(final_output) if final_output else 0}")
                # print(f"[DEBUG] final_output é¢„è§ˆ: {final_output[:200] if final_output else 'None'}...")
                await self.progress_callback({
                    "type": "report_completed",
                    "message": "ç»¼è¿°æ’°å†™å®Œæˆï¼",
                    "report_content": final_output
                })

            return final_output

        except Exception as e:
            traceback.print_exc()
            error_msg = f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"
            await self.webhook_manager.send_error("E5001", error_msg)


async def main():
    # æ–¹å¼1: ä½¿ç”¨é»˜è®¤æµ‹è¯•é…ç½® + å›ºå®šå·¥ä½œåŒºï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
    deep_research = DeepResearchAgent(use_fixed_workspace=True)
    await deep_research.build()
    
    query = "STAT6åœ¨ç‰¹åº”æ€§çš®ç‚ä¸­çš„ç ”ç©¶è¿›å±•"
    print(f"ğŸš€ å¼€å§‹ç ”ç©¶ä»»åŠ¡: {query}")
    print(f"ğŸ†” Task ID: {deep_research.webhook_manager.task_id}")
    print(f"ğŸ’¾ Callback æ—¥å¿—: {deep_research.webhook_manager.log_file}")
    print(f"ğŸŒ HTTP å‘é€: {'å¯ç”¨' if deep_research.webhook_manager.send_http else 'ç¦ç”¨ï¼ˆä»…ä¿å­˜åˆ°æ–‡ä»¶ï¼‰'}")
    print("-" * 80)
    
    
    outline_path = "/Users/fl/Desktop/my_code/DeepResearcher/wide_research/outline.json"
            
    with open(outline_path, "r", encoding='utf-8') as f:
        parsed_outline = json.load(f)
        
    result = await deep_research.run_streamed(parsed_outline)
    
    # ä¿å­˜ç»“æœ
    with open("final_report.md", "w", encoding='utf-8') as f:
        f.write(result)
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° final_report.md")
    print(f"{'-' * 80}\n{result[:500]}...\n{'-' * 80}")
    
    # æ¸…ç†èµ„æºå¹¶æ˜¾ç¤ºå›è°ƒæ€»ç»“
    await deep_research.webhook_manager.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
