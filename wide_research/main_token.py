# pylint: disable=line-too-long
# ruff: noqa: E501
"""
https://manus.im/blog/introducing-wide-research
"""

import asyncio
import json
import pathlib
import traceback
import re
import uuid
import yaml

from agents import function_tool

from utu.agents import SimpleAgent
from utu.config import ConfigLoader
from utu.tools import SearchToolkit
from utu.tools import SerperToolkit
from utu.utils import AgentsUtils, FileUtils, schema_to_basemodel
from utu.utils.Citation import CitationProcessor

PROMPTS = FileUtils.load_yaml(pathlib.Path(__file__).parent / "prompts.yaml")
SEARCH_TOOLKIT = SearchToolkit(ConfigLoader.load_toolkit_config("search"))
SERPER_TOOLKIT = SerperToolkit(ConfigLoader.load_toolkit_config("serper"))
CONCURRENCY = 20


def parse_document(text):
    """
    è§£æ YAML æ ¼å¼çš„æ–‡æ¡£
    
    Args:
        text: YAML æ ¼å¼çš„æ–‡æœ¬
        
    Returns:
        dict: è§£æåçš„å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µ
    """
    try:
        # ç›´æ¥ç”¨ yaml.safe_load è§£æ
        data = yaml.safe_load(text)
        return data if data else {}
    except yaml.YAMLError as e:
        print(f"YAML è§£æé”™è¯¯: {e}")
        return {}

def parse_markdown_outline(markdown_text: str) -> dict:
    """
    Parses a structured Markdown outline into a nested dictionary.
    """
    lines = markdown_text.strip().split('\n')
    
    outline = {"title": "", "sections": []}
    current_section = None
    
    # Use a regex to find FOCUS and KEYWORDS lines
    focus_regex = re.compile(r"^\s*FOCUS:\s*(.*)", re.IGNORECASE)
    keywords_regex = re.compile(r"^\s*KEYWORDS:\s*(.*)", re.IGNORECASE)

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Main Title
        if line.startswith('# '):
            outline["title"] = line[2:].strip()
            continue

        # Section Title
        if line.startswith('## '):
            section_title = line[3:].strip()
            current_section = {
                "title": section_title,
                "id": section_title.split('.')[0],
                "subsections": []
            }
            outline["sections"].append(current_section)
            continue
            
        # Subsection Title
        if line.startswith('### '):
            if current_section is None:
                # Handle case where a subsection appears before a section
                continue
            
            subsection_title = line[4:].strip()
            subsection_data = {
                "title": subsection_title,
                "id": subsection_title.split(' ')[0],
                "research_focus": "",
                "keywords": []
            }
            current_section["subsections"].append(subsection_data)
            continue
            
        # Focus Line
        focus_match = focus_regex.match(line)
        if focus_match:
            if current_section and current_section["subsections"]:
                current_section["subsections"][-1]["research_focus"] = focus_match.group(1).strip()
            continue

        # Keywords Line
        keywords_match = keywords_regex.match(line)
        if keywords_match:
            if current_section and current_section["subsections"]:
                keywords_str = keywords_match.group(1).strip()
                keywords_list = [k.strip() for k in keywords_str.split(',')]
                current_section["subsections"][-1]["keywords"] = keywords_list
            continue
            
    return outline


class DeepResearchAgent:
    def __init__(self):
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tokens': 0,
            'total_searches': 0,
            'clarifier_tokens': 0,
            'planner_tokens': 0,
            'searcher_tokens': 0,
            'writer_tokens': 0,
            'search_counts': {
                'google_search': 0,
                'scholar_search': 0,
                'news_search': 0,
                'web_qa': 0,
                'image_search': 0,
            }
        }
    
    async def build(self):
        # 1. æ¾„æ¸… Agent
        self.clarifier_agent = SimpleAgent(
            name="ClarifierAgent",
            instructions=PROMPTS["clarifier"],
        )
        
        # 2. è§„åˆ’ Agent
        self.planner_agent = SimpleAgent(
            name="PlannerAgent",
            instructions=PROMPTS["planner_new"], # éœ€è¦ä¸€ä¸ªæ–°çš„ Promptï¼ŒæŒ‡å¯¼å®ƒåªç”Ÿæˆå¤§çº²JSON
  
        )
        
        self.searcher_agent = SimpleAgent(
            name="SearcherAgent",
            instructions=PROMPTS["searcher_cn"], # æˆ‘ä»¬å°†åˆ›å»ºè¿™ä¸ªæ–° Prompt
            tools=SEARCH_TOOLKIT.get_tools_in_agents() + SERPER_TOOLKIT.get_tools_in_agents()
        )
        
        # 3. ç« èŠ‚æ’°å†™ Agent (åŸ Formatterï¼Œä½† Prompt èšç„¦äºå•ç« èŠ‚å†™ä½œ)
        self.section_writer_agent = SimpleAgent(
            name="SectionWriterAgent",
            instructions=PROMPTS["section_writer"], # æ–° Promptï¼ŒæŒ‡å¯¼å®ƒæ ¹æ®èµ„æ–™æ’°å†™ç‰¹å®šç« èŠ‚
        )



    async def _clarify_task(self, task: str) -> str:
        """Helper function to handle the clarification step."""
        print("ğŸ” æ­¥éª¤ 1: æ­£åœ¨åˆ†æå¹¶æ¾„æ¸…æ‚¨çš„è°ƒç ”éœ€æ±‚...")
        async with self.clarifier_agent as clarifier:
            clarification_result = await clarifier.run(task)
            run_result = clarification_result.get_run_result()
            clarification_questions = run_result.final_output
            
            # ç»Ÿè®¡ token - ä» raw_responses ä¸­è·å–
            if hasattr(run_result, 'raw_responses') and run_result.raw_responses:
                for resp in run_result.raw_responses:
                    if hasattr(resp, 'usage') and resp.usage:
                        tokens = resp.usage.total_tokens
                        self.stats['clarifier_tokens'] += tokens
                        self.stats['total_tokens'] += tokens
                print(f"  ğŸ’° æ­¥éª¤1æ¶ˆè€— tokens: {self.stats['clarifier_tokens']}")

        if "æ— éœ€æ¾„æ¸…" not in clarification_questions:
            print(f"\nã€æ¾„æ¸…é—®é¢˜ã€‘:\n{clarification_questions}")
            user_clarifications = input("\n> è¯·æ‚¨å›ç­”ä¸Šè¿°é—®é¢˜ä»¥è·å¾—æ›´ç²¾å‡†çš„æŠ¥å‘Š (æˆ–ç›´æ¥æŒ‰å›è½¦è·³è¿‡): ")
            if user_clarifications.strip():
                # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ promptï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                merge_prompt = PROMPTS["clarifier_merge"].format(task=task, clarification_questions=clarification_questions, user_clarifications=user_clarifications)
                # å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„Agentæˆ–LLMè°ƒç”¨æ¥å®Œæˆæ•´åˆ
                # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥æ‹¼æ¥
                enhanced_task = merge_prompt
                print("\nâœ… æ„Ÿè°¢æ‚¨çš„æ¾„æ¸…ï¼å·²æ›´æ–°ç ”ç©¶ä»»åŠ¡ã€‚")
            else:
                enhanced_task = task
                print("\nâ© å·²è·³è¿‡æ¾„æ¸…ï¼Œå°†æŒ‰åŸä»»åŠ¡æ‰§è¡Œã€‚")
        else:
            enhanced_task = task
            print("âœ… ä»»åŠ¡æ¸…æ™°ï¼Œæ— éœ€æ¾„æ¸…ã€‚")
        
        return enhanced_task

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“Š è¿è¡Œç»Ÿè®¡ä¿¡æ¯ (Statistics)")
        print("="*80)
        
        # Token ç»Ÿè®¡
        print(f"\nğŸ’° Token æ¶ˆè€—ç»Ÿè®¡:")
        print(f"  - æ¾„æ¸…é˜¶æ®µ (Clarifier):     {self.stats['clarifier_tokens']:>10,} tokens")
        print(f"  - è§„åˆ’é˜¶æ®µ (Planner):       {self.stats['planner_tokens']:>10,} tokens")
        print(f"  - æœç´¢é˜¶æ®µ (Searcher):      {self.stats['searcher_tokens']:>10,} tokens")
        print(f"  - å†™ä½œé˜¶æ®µ (Writer):        {self.stats['writer_tokens']:>10,} tokens")
        print(f"  {'â”€'*60}")
        print(f"  - æ€»è®¡ (Total):             {self.stats['total_tokens']:>10,} tokens")
        
        # æ£€ç´¢ç»Ÿè®¡
        print(f"\nğŸ” æ£€ç´¢æ¬¡æ•°ç»Ÿè®¡:")
        
        # æ˜¾ç¤ºæ‰€æœ‰è¢«è°ƒç”¨çš„å·¥å…·ï¼ˆåŒ…æ‹¬åŠ¨æ€æ·»åŠ çš„ï¼‰
        tool_display_names = {
            'google_search': 'Google æœç´¢',
            'scholar_search': 'å­¦æœ¯æœç´¢',
            'news_search': 'æ–°é—»æœç´¢',
            'web_qa': 'ç½‘é¡µå†…å®¹è·å–',
            'image_search': 'å›¾åƒæœç´¢',
            'search': 'æœç´¢',
            'video_search': 'è§†é¢‘æœç´¢',
            'map_search': 'åœ°å›¾æœç´¢',
            'place_search': 'åœ°ç‚¹æœç´¢',
            'autocomplete': 'è‡ªåŠ¨è¡¥å…¨',
            'google_lens': 'Google Lens',
        }
        
        # æŒ‰è°ƒç”¨æ¬¡æ•°æ’åºæ˜¾ç¤º
        sorted_tools = sorted(self.stats['search_counts'].items(), key=lambda x: x[1], reverse=True)
        for tool_name, count in sorted_tools:
            display_name = tool_display_names.get(tool_name, tool_name)
            if count > 0:  # åªæ˜¾ç¤ºå®é™…è¢«è°ƒç”¨çš„å·¥å…·
                print(f"  - {display_name:24} {count:>10} æ¬¡")
        
        print(f"  {'â”€'*60}")
        print(f"  - æ€»è®¡æ£€ç´¢æ¬¡æ•°:             {self.stats['total_searches']:>10} æ¬¡")
        
        # æˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾ä½¿ç”¨ GPT-4ï¼‰
        # GPT-4 ä»·æ ¼: $0.03/1K input tokens, $0.06/1K output tokens
        # ç®€åŒ–è®¡ç®—ï¼Œå‡è®¾è¾“å…¥è¾“å‡ºå„å ä¸€åŠ
        estimated_cost = (self.stats['total_tokens'] / 1000) * 0.045  # å¹³å‡å€¼
        print(f"\nğŸ’µ ä¼°ç®—æˆæœ¬ (åŸºäº GPT-4 ä»·æ ¼):")
        print(f"  - çº¦ ${estimated_cost:.2f} USD")
        print(f"  - çº¦ Â¥{estimated_cost * 7:.2f} CNY (æŒ‰ 1:7 æ±‡ç‡)")
        
        print("="*80 + "\n")
    
    def process_draft(self, draft_content: str, metadata: dict) -> str:
        """
        å¤„ç†åˆç¨¿ï¼Œæ’å…¥æ–‡çŒ®å¼•ç”¨
        
        Args:
            draft_content: åˆç¨¿å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
            metadata: å‚è€ƒæ–‡çŒ®å…ƒæ•°æ®å­—å…¸
            
        Returns:
            å¤„ç†åçš„å®Œæ•´æ–‡æ¡£ï¼ˆåŒ…å«å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼‰
        """
        processor = CitationProcessor(metadata)
        processed_content, reference_list = processor.process(draft_content)
        
        # ç»„è£…æœ€ç»ˆæ–‡æ¡£
        final_document = f"{processed_content}\n\n## å‚è€ƒæ–‡çŒ® (References)\n\n{reference_list}"
        
        return final_document

    async def run_streamed(self, task: str):
        """
        Orchestrates the entire research and writing process from planning to final report.
        """
        # === æ­¥éª¤ 0: åˆå§‹åŒ–å·¥ä½œåŒº ===
        project_id = f"research_{uuid.uuid4().hex[:8]}"
        workspace_dir = pathlib.Path(__file__).parent / "workspace" / project_id
        workspace_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸš€ åˆå§‹åŒ–æˆåŠŸï¼ä¸ºæœ¬æ¬¡ç ”ç©¶åˆ›å»ºä¸“å±å·¥ä½œåŒº: {workspace_dir}")

        try:
            # === æ­¥éª¤ 1: ä»»åŠ¡æ¾„æ¸… ===
            enhanced_task = await self._clarify_task(task)

            # === æ­¥éª¤ 2: ç”Ÿæˆç ”ç©¶å¤§çº² ===
            print("\nğŸ“‹ æ­¥éª¤ 2: æ­£åœ¨ç”Ÿæˆç ”ç©¶å¤§çº²...")
            async with self.planner_agent as planner:
                plan_result = planner.run_streamed(enhanced_task)
                await AgentsUtils.print_stream_events(plan_result.stream_events())
                markdown_outline = plan_result.final_output
                
                # ç»Ÿè®¡ token - ä» raw_responses ä¸­è·å–
                if hasattr(plan_result, 'raw_responses') and plan_result.raw_responses:
                    for resp in plan_result.raw_responses:
                        if hasattr(resp, 'usage') and resp.usage:
                            tokens = resp.usage.total_tokens
                            self.stats['planner_tokens'] += tokens
                            self.stats['total_tokens'] += tokens
                    print(f"  ğŸ’° æ­¥éª¤2æ¶ˆè€— tokens: {self.stats['planner_tokens']}")
            
            # æŒä¹…åŒ–å¤§çº²
            outline_path = workspace_dir / "outline.md"
            outline_path.write_text(markdown_outline, encoding='utf-8')
            print(f"  ğŸ’¾ å¤§çº²å·²ä¿å­˜åˆ°: {outline_path}")

            parsed_outline = parse_markdown_outline(markdown_outline)
            
            # æŒä¹…åŒ–è§£æåçš„å¤§çº²JSONï¼Œä¾¿äºè°ƒè¯•
            parsed_outline_path = workspace_dir / "outline.json"
            with parsed_outline_path.open('w', encoding='utf-8') as f:
                json.dump(parsed_outline, f, indent=2, ensure_ascii=False)
            print(f"  âœ… å¤§çº²è§„åˆ’ä¸è§£æå®Œæˆï¼å…±è§„åˆ’äº† {len(parsed_outline.get('sections', []))} ä¸ªç« èŠ‚ã€‚")

            # === æ­¥éª¤ 3: è¿­ä»£å¼ç ”ç©¶ä¸æ’°å†™ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰ ===
            written_sections = []
            all_sources_metadata = {}

            # æ”¶é›†æ‰€æœ‰å­ç« èŠ‚
            all_subsections = []
            for section in parsed_outline.get("sections", []):
                for subsection in section.get("subsections", []):
                    subsection['sections_title'] = section['title']
                    subsection['sections_id'] = section['id']
                    
                    all_subsections.append(subsection)
            
            print(f"\nğŸ“Š å‡†å¤‡å¹¶è¡Œå¤„ç† {len(all_subsections)} ä¸ªç« èŠ‚...")
            
            # å®šä¹‰å•ä¸ªå­ç« èŠ‚çš„å¤„ç†å‡½æ•°
            async def process_subsection(subsection, semaphore):
                async with semaphore:
                    sub_id = subsection['id'].replace('.', '_')
                    print(f"\n{'='*25} å¼€å§‹å¤„ç†ç« èŠ‚ {subsection['id']}: {subsection['title']} {'='*25}")
                    
                    try:
                        # --- 3a. æ‰§è¡Œæ·±åº¦æœç´¢ ---
                        print(f"  ğŸ” [{subsection['id']}] (3a) æœç´¢ä¸­...")
                        searcher_prompt = f"""
                        ### **ç ”ç©¶ç„¦ç‚¹ (Research Focus)**:
                        {subsection['research_focus']}
                        
                        ### **å…³é”®è¯ (Keywords)**:
                        {", ".join(subsection['keywords'])}
                        """
                        
                        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ agent å®ä¾‹
                        async with self.searcher_agent as searcher:
                            result_stream = await searcher.run(searcher_prompt)
                            run_result = result_stream.get_run_result()
                            markdown_data = run_result.final_output
                            
                            # ç»Ÿè®¡ token - ä» raw_responses ä¸­è·å–
                            chapter_tokens = 0
                            if hasattr(run_result, 'raw_responses') and run_result.raw_responses:
                                for resp in run_result.raw_responses:
                                    if hasattr(resp, 'usage') and resp.usage:
                                        tokens = resp.usage.total_tokens
                                        chapter_tokens += tokens
                                        self.stats['searcher_tokens'] += tokens
                                        self.stats['total_tokens'] += tokens
                                print(f"  ğŸ’° [{subsection['id']}] æœç´¢æ¶ˆè€— tokens: {chapter_tokens}")
                            
                            # ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•° - ä» new_items ä¸­è·å–
                            if hasattr(run_result, 'new_items'):
                                from agents import ToolCallItem
                                for item in run_result.new_items:
                                    if isinstance(item, ToolCallItem):
                                        tool_name = item.raw_item.name if hasattr(item.raw_item, 'name') else str(item)
                                        # è°ƒè¯•ï¼šæ‰“å°å·¥å…·åç§°
                                        print(f"    ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_name}")
                                        if tool_name in self.stats['search_counts']:
                                            self.stats['search_counts'][tool_name] += 1
                                            self.stats['total_searches'] += 1
                                        else:
                                            # å·¥å…·åç§°ä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­ï¼Œéœ€è¦åŠ¨æ€æ·»åŠ 
                                            print(f"    âš ï¸ æœªçŸ¥å·¥å…·ç±»å‹: {tool_name}")
                                            if tool_name not in self.stats['search_counts']:
                                                self.stats['search_counts'][tool_name] = 0
                                            self.stats['search_counts'][tool_name] += 1
                                            self.stats['total_searches'] += 1
                        
                        (workspace_dir / "data").mkdir(exist_ok=True)
                        search_data_path = workspace_dir / "data" / f"{sub_id}_raw_data.md"
                        search_data_path.write_text(markdown_data, encoding='utf-8')
                        print(f"    ğŸ’¾ [{subsection['id']}] åŸå§‹æœç´¢æ•°æ®å·²ä¿å­˜: {search_data_path}")
                        
                        # --- 3b. è§£ææœç´¢ç»“æœ ---
                        print(f"  ğŸ§© [{subsection['id']}] (3b) è§£ææœç´¢ç»“æœ...")
                        
                        raw_posts = [p.strip() for p in re.split(r'----', markdown_data) if p.strip()]
                        context_for_writer = ""
                        
                        if not raw_posts:
                            print(f"    âš ï¸ [{subsection['id']}] è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æ–‡çŒ®ã€‚å°†ä¸ºæ­¤ç« èŠ‚ç”Ÿæˆå ä½å†…å®¹ã€‚")
                            return {
                                'sections_id': subsection['sections_id'],
                                'sections_title': subsection['sections_title'],
                                "id": subsection['id'], 
                                "title": subsection['title'], 
                                "content": "ã€ç¼–è€…æ³¨ï¼šæœªèƒ½æ‰¾åˆ°ç›¸å…³å‚è€ƒæ–‡çŒ®ï¼Œæ­¤ç« èŠ‚å†…å®¹ç¼ºå¤±ã€‚ã€‘",
                                "order": subsection['id']
                            }
                        
                        count = 0
                        for post_str in raw_posts:
                            post_str = post_str.replace('----', '')
                            post = parse_document(post_str)
                            print('post:',post)
                            print('-'*100)
                            try:
                                post = parse_document(post_str)
                                if (ck := post.get('citation_key')):
                                    all_sources_metadata[ck] = post
                                    context_for_writer += f"### æ–‡çŒ®æ¥æº: {ck}\n**æ ‡é¢˜**: {post.get('title', 'N/A')}\n\n{post.get('content', 'N/A')}\n\n"
                                    count += 1
                            except Exception:
                                print(f"    - [{subsection['id']}] è§£ææŸç¯‡æ–‡çŒ®å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                        
                        print(f"    âœ… [{subsection['id']}] è§£æå®Œæˆï¼Œå¤„ç†äº† {count} ç¯‡æ–‡çŒ®ã€‚")
                        
                        # --- 3c. æ’°å†™ç« èŠ‚å†…å®¹ ---
                        print(f"  âœï¸  [{subsection['id']}] (3c) æ’°å†™ç« èŠ‚å†…å®¹...")
                        writer_prompt = PROMPTS["section_writer"].format(
                            subsection_title=subsection['title'],
                            subsection_focus=subsection['research_focus'],
                            context_for_writer=context_for_writer
                        )
                        
                        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ writer å®ä¾‹
                        writer = SimpleAgent(
                            name=f"SectionWriterAgent_{sub_id}",
                            instructions=PROMPTS["section_writer"],
                        )
                        
                        async with writer:
                            section_result = writer.run_streamed(writer_prompt)
                            await AgentsUtils.print_stream_events(section_result.stream_events())
                            written_content = section_result.final_output
                            
                            # ç»Ÿè®¡ writer token - ä» raw_responses ä¸­è·å–
                            chapter_writer_tokens = 0
                            if hasattr(section_result, 'raw_responses') and section_result.raw_responses:
                                for resp in section_result.raw_responses:
                                    if hasattr(resp, 'usage') and resp.usage:
                                        tokens = resp.usage.total_tokens
                                        chapter_writer_tokens += tokens
                                        self.stats['writer_tokens'] += tokens
                                        self.stats['total_tokens'] += tokens
                                print(f"  ğŸ’° [{subsection['id']}] å†™ä½œæ¶ˆè€— tokens: {chapter_writer_tokens}")
                        
                        (workspace_dir / "drafts").mkdir(exist_ok=True)
                        draft_path = workspace_dir / "drafts" / f"{sub_id}_draft.md"
                        draft_path.write_text(written_content, encoding='utf-8')
                        print(f"    ğŸ’¾ [{subsection['id']}] ç« èŠ‚è‰ç¨¿å·²ä¿å­˜: {draft_path}")
                        
                        print(f"âœ… [{subsection['id']}] ç« èŠ‚å¤„ç†å®Œæˆï¼")
                        return {
                            'sections_id': subsection['sections_id'],
                            'sections_title': subsection['sections_title'],
                            "id": subsection['id'], 
                            "title": subsection['title'], 
                            "content": written_content,
                            "order": subsection['id']
                        }
                    
                    except Exception as e:
                        print(f"âŒ [{subsection['id']}] å¤„ç†å‡ºé”™: {e}")
                        traceback.print_exc()
                        return {
                            'sections_id': subsection['sections_id'],
                            'sections_title': subsection['sections_title'],
                            "id": subsection['id'], 
                            "title": subsection['title'], 
                            "content": f"ã€ç¼–è€…æ³¨ï¼šå¤„ç†æ­¤ç« èŠ‚æ—¶å‡ºé”™: {str(e)}ã€‘",
                            "order": subsection['id']
                        }
            
            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(CONCURRENCY)
            
            # å¹¶è¡Œå¤„ç†æ‰€æœ‰å­ç« èŠ‚
            tasks = [process_subsection(subsection, semaphore) for subsection in all_subsections]
            results = await asyncio.gather(*tasks)
            
            # æŒ‰ç…§åŸå§‹é¡ºåºæ’åºç»“æœ
            written_sections = sorted(results, key=lambda x: x['order'])
            

            # === æ­¥éª¤ 4: æœ€ç»ˆæ•´åˆä¸å®¡é˜… ===
            print(f"\n{"="*25} æ­¥éª¤ 4: æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ•´åˆä¸å®¡é˜… {'='*25}")

            # å‡†å¤‡æœ€ç»ˆ prompt æ‰€éœ€çš„ææ–™
            title = parsed_outline.get("title", "æœªå‘½åç»¼è¿°")
            full_content = f"# {title}\n\n"
            current_section_id = None # ç”¨äºè¿½è¸ªå½“å‰çš„ç« èŠ‚ID

            for section in written_sections:
                # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº†ä¸€ä¸ªæ–°çš„å¤§ç« èŠ‚
                if section['sections_id'] != current_section_id:
                    full_content += f"# {section['sections_title']}\n\n"
                    current_section_id = section['sections_id'] # æ›´æ–°å½“å‰ç« èŠ‚ID

                # æ ¹æ®æ˜¯å¦ä¸ºç¬¬ä¸€ç« ï¼Œå†³å®šå¦‚ä½•æ·»åŠ å†…å®¹
                if current_section_id == '1':
                    # ç¬¬ä¸€ç« ï¼šä¸æ·»åŠ å°æ ‡é¢˜
                    full_content += f"{section['content']}\n\n"
                else:
                    # å…¶ä»–ç« èŠ‚ï¼šæ·»åŠ å°æ ‡é¢˜
                    full_content += f"### {section['title']}\n{section['content']}\n\n"
        
            all_sources_metadata_json = json.dumps(all_sources_metadata, indent=2, ensure_ascii=False)
            
            # æŒä¹…åŒ–æ•´åˆå‰çš„ææ–™
            (workspace_dir / "final_inputs").mkdir(exist_ok=True)
            (workspace_dir / "final_inputs" / "full_content.md").write_text(full_content, encoding='utf-8')
            (workspace_dir / "final_inputs" / "all_metadata.json").write_text(all_sources_metadata_json, encoding='utf-8')

            
            final_output = self.process_draft(full_content, all_sources_metadata)

            
            # æŒä¹…åŒ–æœ€ç»ˆæŠ¥å‘Š
            final_report_path = workspace_dir / "final_report.md"
            final_report_path.write_text(final_output, encoding='utf-8')
            print(f"  ğŸ’¾ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {final_report_path}")

            print("\nğŸ‰ğŸ‰ğŸ‰ ç ”ç©¶ç»¼è¿°ç”Ÿæˆå®Œæ¯•ï¼ ğŸ‰ğŸ‰ğŸ‰")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.print_statistics()
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶
            stats_path = workspace_dir / "statistics.json"
            with stats_path.open('w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")
            
            return final_output

        except Exception as e:
            print(f"\nâŒ åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            traceback.print_exc()
            # å³ä½¿å‡ºé”™ä¹Ÿæ˜¾ç¤ºå·²æ”¶é›†çš„ç»Ÿè®¡ä¿¡æ¯
            self.print_statistics()
            return f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ã€‚è¯·æ£€æŸ¥å·¥ä½œåŒºæ–‡ä»¶å¤¹ '{workspace_dir}' ä¸­çš„æ—¥å¿—å’Œä¸­é—´æ–‡ä»¶ä»¥è¿›è¡Œè°ƒè¯•ã€‚"


async def main():
    deep_research = DeepResearchAgent()
    await deep_research.build()
    query = input("What would you like to research? ")
    query = query.strip() or TASK
    print(f"Processing task: {query}")
    result = await deep_research.run_streamed(query)

    with open("final_report.md", "w") as f:
        f.write(result)
    print(f"{'-' * 80}\n{result}\n{'-' * 80}")


if __name__ == "__main__":
    asyncio.run(main())
